{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reference: Chapter 4.2.1 of https://project-archive.inf.ed.ac.uk/msc/20204379/msc_proj.pdf\n",
    "\n",
    "__Linear Gaussian State Space Model__\n",
    "$$\n",
    "\\begin{aligned}\n",
    "p(x_1) & = \\mathcal{N}_{x_1}(\\mathbf{0}, \\mathbf{I}) \\\\\n",
    "f(x_t | x_{t-1}) & = \\mathcal{N}_{x_t}(Ax_{t-1}+c, \\mathbf{R}) \\\\\n",
    "g(y_t | x_t) & = \\mathcal{N}_{y_t}(Cx_t+g, \\mathbf{Q}) \\\\\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, pickle\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch.distributions import MultivariateNormal\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "sys.path.append(os.path.dirname(os.getcwd()))\n",
    "\n",
    "from src.networks import MLP\n",
    "from src.utils import assemble_grad_potential, langevin_sampler\n",
    "\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "dim = 1\n",
    "I = torch.eye(dim, device=device)\n",
    "zeros = torch.zeros((dim,), device=device)\n",
    "A = I\n",
    "C = I\n",
    "c = 0.\n",
    "g = 0.\n",
    "R = 5 * I\n",
    "Q = 0.2 * I\n",
    "Q_inv = torch.linalg.inv(Q)\n",
    "mu1 = torch.zeros((dim, ), device=device)\n",
    "cov1 = I\n",
    "T = 10\n",
    "\n",
    "DynamicsNoiseGenerator = MultivariateNormal(loc=zeros, covariance_matrix=R)\n",
    "ObservationNoiseGenerator = MultivariateNormal(loc=zeros, covariance_matrix=Q)\n",
    "\n",
    "def transition(x):\n",
    "    loc = x @ A + c\n",
    "    return loc + DynamicsNoiseGenerator.sample((x.shape[0],))\n",
    "\n",
    "def observation(x):\n",
    "    loc = x @ C + g\n",
    "    return loc + ObservationNoiseGenerator.sample((x.shape[0],))\n",
    "\n",
    "def score_likelihood_fn(x, y):\n",
    "    return (y - x @ C - g) @ C.T @ Q_inv\n",
    "\n",
    "x = torch.empty((T, dim), device=device)\n",
    "x_true = torch.empty((T, dim), device=device)\n",
    "y = torch.empty((T, dim), device=device)\n",
    "x[0] = mu1\n",
    "for i in range(T-1):\n",
    "    x[i+1:i+2] = transition(x[i:i+1])\n",
    "\n",
    "for i in range(T):\n",
    "    y[i:i+1] = observation(x[i:i+1])\n",
    "\n",
    "mu = mu1\n",
    "cov = cov1\n",
    "mu_post = torch.empty((T, dim), device=device)\n",
    "cov_post = torch.empty((T, dim, dim), device=device)\n",
    "for i in range(T):\n",
    "    # prediction\n",
    "    mu_hat = A @ mu + c\n",
    "    cov_hat = A @ cov @ A.T + R\n",
    "    # Kalman Gain\n",
    "    K = cov_hat @ C.T @ torch.linalg.inv(C @ cov_hat @ C.T + Q)\n",
    "    # residual\n",
    "    residual = y[i] - C @ mu_hat\n",
    "    # update\n",
    "    mu = mu_hat + K @ residual\n",
    "    cov = (I - K @ C) @ cov_hat\n",
    "    mu_post[i] = mu\n",
    "    cov_post[i] = cov\n",
    "\n",
    "print(x[-1], mu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workdir = \"../linear_results/exact_prior\"\n",
    "os.makedirs(workdir, exist_ok=True)\n",
    "ntrain = 500\n",
    "denoising_sigma = 0.2\n",
    "lr = 1e-3\n",
    "nepoch = 1000\n",
    "lmc_steps = 1000\n",
    "lmc_stepsize = 1e-3\n",
    "anneal_init = 1e-3\n",
    "anneal_decay = 0.5\n",
    "anneal_steps = 1\n",
    "\n",
    "mu0 = 0.\n",
    "std0 = 1.\n",
    "\n",
    "prior = torch.randn((ntrain, dim), device=device) * std0 + mu0\n",
    "model = MLP(dim=dim, widths=[32, 64], use_bn=True).to(device)\n",
    "assimilated_states = torch.empty((T, ntrain, dim), device=device)\n",
    "\n",
    "with tqdm(range(T), maxinterval=50.0, desc=\"state step\", file=sys.stdout) as pbar:\n",
    "    for i in pbar:\n",
    "        prior_mean, prior_std = prior.mean(dim=0), prior.std(dim=0)\n",
    "        # normalize states for stable input to the network\n",
    "        normalized_prior = (prior - prior_mean) / prior_std\n",
    "        # model predicts the noise from the noised normalized state, as in DDPM\n",
    "        # normalized_score_fn predicts the score for the normalized state\n",
    "        normalized_score_fn = lambda x: -model(x) / denoising_sigma\n",
    "\n",
    "        # denoising score matching\n",
    "        model.train()\n",
    "        optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
    "        dataset = TensorDataset(normalized_prior)\n",
    "        loader = DataLoader(dataset, batch_size=ntrain, shuffle=True)\n",
    "        for _ in range(nepoch):\n",
    "            for batch_no, batch in enumerate(loader, start=1):\n",
    "                (x0,) = batch\n",
    "                x0 = x0.to(device)  # (B, *state_shape)\n",
    "                z = torch.randn_like(x0, device=device)\n",
    "                xt = x0 + denoising_sigma * z\n",
    "                score = normalized_score_fn(xt)\n",
    "                loss = nn.MSELoss()(score * denoising_sigma, -z)\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "        model.eval()\n",
    "\n",
    "        # Posterior sampling and prior update\n",
    "        # score_fn predicts the score for the original (unnormalized) states\n",
    "        # Y = aX + b => s_X(x) = a s_Y(ax+b)\n",
    "        score_fn = lambda x: normalized_score_fn((x - prior_mean) / prior_std) / prior_std\n",
    "        # potential gradient = - score_likelihood - score_prior\n",
    "        grad_potential_fn = assemble_grad_potential(\n",
    "            y=y[i],\n",
    "            score_likelihood=score_likelihood_fn,\n",
    "            score_prior=score_fn,\n",
    "        )\n",
    "\n",
    "        # Debugging scales between likelihood and prior\n",
    "        with torch.no_grad():\n",
    "            score_likelihood = score_likelihood_fn(prior, y[i])\n",
    "            score_likelihood = torch.mean(score_likelihood**2)\n",
    "            score_prior = score_fn(prior)\n",
    "            score_prior = torch.mean(score_prior**2)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            posterior = langevin_sampler(\n",
    "                grad_potential_fn=grad_potential_fn,\n",
    "                x=prior,\n",
    "                steps=lmc_steps,\n",
    "                dt=lmc_stepsize,\n",
    "                anneal_init=anneal_init,\n",
    "                anneal_decay=anneal_decay,\n",
    "                anneal_steps=anneal_steps,\n",
    "            )  # (n_train, *shape)\n",
    "            assimilated_states[i] = posterior\n",
    "            prior = transition(posterior)\n",
    "\n",
    "        \"\"\"Postprocessing.\"\"\"\n",
    "        mean_estimation = torch.mean(posterior, dim=0)  # (*shape, )\n",
    "        median_estimation = torch.median(posterior, dim=0)[0]  # (*shape, )\n",
    "        mean_rmse = torch.sqrt(torch.mean((mean_estimation - x[i]) ** 2))\n",
    "        median_rmse = torch.sqrt(torch.mean((median_estimation - x[i]) ** 2))\n",
    "        pbar.set_postfix(\n",
    "            {\n",
    "                \"mean(RMSE)\": mean_rmse.item(),\n",
    "                \"median(RMSE)\": median_rmse.item(),\n",
    "            },\n",
    "            refresh=False,\n",
    "        )\n",
    "\n",
    "np.savez(\n",
    "    os.path.join(workdir, \"results.npz\"),\n",
    "    assimilated_states=assimilated_states.cpu().numpy(),  # (steps, ntrain, d)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(assimilated_states.cpu().numpy().mean(axis=1).squeeze()) \n",
    "print(mu_post.cpu().numpy().squeeze())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(assimilated_states.cpu().numpy().std(axis=1).squeeze())\n",
    "print((cov_post.cpu().numpy()**0.5).squeeze())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workdir = \"../linear_results/inexact_prior\"\n",
    "os.makedirs(workdir, exist_ok=True)\n",
    "mu0 = -10.\n",
    "std0 = 1.\n",
    "\n",
    "prior = torch.randn((ntrain, dim), device=device) * std0 + mu0\n",
    "model = MLP(dim=dim, widths=[32, 64], use_bn=True).to(device)\n",
    "assimilated_states = torch.empty((T, ntrain, dim), device=device)\n",
    "\n",
    "with tqdm(range(T), maxinterval=50.0, desc=\"state step\", file=sys.stdout) as pbar:\n",
    "    for i in pbar:\n",
    "        prior_mean, prior_std = prior.mean(dim=0), prior.std(dim=0)\n",
    "        # normalize states for stable input to the network\n",
    "        normalized_prior = (prior - prior_mean) / prior_std\n",
    "        # model predicts the noise from the noised normalized state, as in DDPM\n",
    "        # normalized_score_fn predicts the score for the normalized state\n",
    "        normalized_score_fn = lambda x: -model(x) / denoising_sigma\n",
    "\n",
    "        # denoising score matching\n",
    "        model.train()\n",
    "        optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
    "        dataset = TensorDataset(normalized_prior)\n",
    "        loader = DataLoader(dataset, batch_size=ntrain, shuffle=True)\n",
    "        for _ in range(nepoch):\n",
    "            for batch_no, batch in enumerate(loader, start=1):\n",
    "                (x0,) = batch\n",
    "                x0 = x0.to(device)  # (B, *state_shape)\n",
    "                z = torch.randn_like(x0, device=device)\n",
    "                xt = x0 + denoising_sigma * z\n",
    "                score = normalized_score_fn(xt)\n",
    "                loss = nn.MSELoss()(score * denoising_sigma, -z)\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "        model.eval()\n",
    "\n",
    "        # Posterior sampling and prior update\n",
    "        # score_fn predicts the score for the original (unnormalized) states\n",
    "        # Y = aX + b => s_X(x) = a s_Y(ax+b)\n",
    "        score_fn = lambda x: normalized_score_fn((x - prior_mean) / prior_std) / prior_std\n",
    "        # potential gradient = - score_likelihood - score_prior\n",
    "        grad_potential_fn = assemble_grad_potential(\n",
    "            y=y[i],\n",
    "            score_likelihood=score_likelihood_fn,\n",
    "            score_prior=score_fn,\n",
    "        )\n",
    "\n",
    "        # Debugging scales between likelihood and prior\n",
    "        with torch.no_grad():\n",
    "            score_likelihood = score_likelihood_fn(prior, y[i])\n",
    "            score_likelihood = torch.mean(score_likelihood**2)\n",
    "            score_prior = score_fn(prior)\n",
    "            score_prior = torch.mean(score_prior**2)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            posterior = langevin_sampler(\n",
    "                grad_potential_fn=grad_potential_fn,\n",
    "                x=prior,\n",
    "                steps=lmc_steps,\n",
    "                dt=lmc_stepsize,\n",
    "                anneal_init=anneal_init,\n",
    "                anneal_decay=anneal_decay,\n",
    "                anneal_steps=anneal_steps,\n",
    "            )  # (n_train, *shape)\n",
    "            assimilated_states[i] = posterior\n",
    "            prior = transition(posterior)\n",
    "\n",
    "        \"\"\"Postprocessing.\"\"\"\n",
    "        mean_estimation = torch.mean(posterior, dim=0)  # (*shape, )\n",
    "        median_estimation = torch.median(posterior, dim=0)[0]  # (*shape, )\n",
    "        mean_rmse = torch.sqrt(torch.mean((mean_estimation - x[i]) ** 2))\n",
    "        median_rmse = torch.sqrt(torch.mean((median_estimation - x[i]) ** 2))\n",
    "        pbar.set_postfix(\n",
    "            {\n",
    "                \"mean(RMSE)\": mean_rmse.item(),\n",
    "                \"median(RMSE)\": median_rmse.item(),\n",
    "            },\n",
    "            refresh=False,\n",
    "        )\n",
    "\n",
    "np.savez(\n",
    "    os.path.join(workdir, \"results.npz\"),\n",
    "    assimilated_states=assimilated_states.cpu().numpy(),  # (steps, ntrain, d)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(assimilated_states.cpu().numpy().mean(axis=1).squeeze()) \n",
    "print(mu_post.cpu().numpy().squeeze())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(assimilated_states.cpu().numpy().std(axis=1).squeeze())\n",
    "print((cov_post.cpu().numpy()**0.5).squeeze())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workdirs = [\"../linear_results/exact_prior\", \"../linear_results/inexact_prior\"]\n",
    "states = []\n",
    "for workdir in workdirs:\n",
    "    data = np.load(os.path.join(workdir, \"results.npz\"))\n",
    "    states.append(data[\"assimilated_states\"])\n",
    "\n",
    "with open(\"../asset/linear.pkl\", \"wb\") as file:\n",
    "    pickle.dump((states, mu_post.cpu(), cov_post.cpu()), file)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../asset/linear.pkl\", \"rb\") as file:\n",
    "    states, mu_post, cov_post = pickle.load(file)\n",
    "mpl.rcdefaults()\n",
    "mpl.style.use('../configs/mplrc')\n",
    "mpl.rc(\"figure.subplot\", wspace=0.35, hspace=0.7)\n",
    "\n",
    "def normal_density(x, mu, cov):\n",
    "    d = mu.shape[0]\n",
    "    tmp = np.einsum('nd,dd,nd->n', x-mu, np.linalg.inv(cov), x-mu)\n",
    "    return (2*np.pi)**(-d/2) * np.linalg.det(cov)**(-0.5) * np.exp(-0.5*tmp)\n",
    "\n",
    "mu1 = np.zeros(dim)\n",
    "mu1_shift = np.ones(dim) * -10\n",
    "cov1 = np.eye(dim)\n",
    "ntrain = 500\n",
    "\n",
    "nrows = 2\n",
    "ncols = 4\n",
    "fig, axes = plt.subplots(\n",
    "    nrows=nrows, \n",
    "    ncols=ncols, \n",
    "    figsize=(7, 2.5)\n",
    "    )\n",
    "\n",
    "sns.histplot(np.random.multivariate_normal(mu1, cov1, 500).squeeze(), ax=axes[0][0], color='C1', legend=False, stat='density')\n",
    "sns.histplot(np.random.multivariate_normal(mu1_shift, cov1, 500).squeeze(), ax=axes[1][0], color='C1', legend=False, stat='density')\n",
    "\n",
    "low, high = mu1 - 4*cov1[0]**0.5, mu1 + 4*cov1[0]**0.5\n",
    "t = np.linspace(low, high, 100)\n",
    "p = normal_density(t, mu1, cov1)\n",
    "axes[0][0].plot(t, p, color='C0')\n",
    "axes[0][0].set_xlim((low, high))\n",
    "axes[1][0].plot(t, p, color='C0')\n",
    "\n",
    "for assimilated_states, ax in zip(states, axes):\n",
    "    for states, axi in zip(assimilated_states[::2], ax[1:]):\n",
    "        sns.histplot(states.squeeze(), ax=axi, color='C1', legend=False, stat='density')\n",
    "\n",
    "\n",
    "for mu, cov, ax1, ax2 in zip(mu_post[::2], cov_post[::2], axes[0][1:], axes[1][1:]):\n",
    "    low, high = mu - 4*cov[0]**0.5, mu + 4*cov[0]**0.5\n",
    "    t = np.linspace(low, high, 100)\n",
    "    p = normal_density(t, mu, cov)\n",
    "    for ax in ax1, ax2:\n",
    "        ax.plot(t, p, color='C0')\n",
    "        ax.set_xlim((low, high))\n",
    "\n",
    "for ax in axes.flat:\n",
    "    ax.set_ylabel('')\n",
    "\n",
    "axes[0][0].set_ylabel('Assimilation with\\nexact prior')\n",
    "axes[1][0].set_ylabel('Assimilation with\\ninexact prior')\n",
    "\n",
    "for ax, title in zip(axes[1], ['Initial prior', '1st time step', '3rd time step', '5th time step']):\n",
    "    ax.set_xlabel(title)\n",
    "\n",
    "custom_lines = [\n",
    "    mpl.lines.Line2D([0], [0], color='C0', label='Ground truth posterior density'),\n",
    "    mpl.patches.Patch(facecolor='C1', label='SSLS ensemble histogram')\n",
    "]\n",
    "axes[0][0].legend(handles=custom_lines, bbox_to_anchor=(0.3, 1.0), loc='lower left', ncol=2)\n",
    "\n",
    "plt.savefig('../asset/Linear.pdf', dpi=600, bbox_inches='tight', pad_inches=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
